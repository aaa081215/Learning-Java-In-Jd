# kafka消费端数据积压的几种特殊情况及解决方案
## 讨论范围
本文档只讨论和kafka消费有关的几种特殊积压场景，不涉及消费端本身性能或业务逻辑原因导致的积压情况。
## 特殊场景数据积压出现前提条件
+ kafka消费端使用主动式ack（或者JDQ高级消费者）
+ 问题出现前消费正常无积压
+ 未做过位点重置操作
+ kafka消费端计算一切正常，tps，提交等各指标无异常
+ kafka监控系统显示topic总体上有大量数据积压（或者通过对比位点发现最近提交位点相对最新位点差异极大）  

## 几种特殊的数据积压场景
#### 单分区积压或个别分区积压
- 线上现象
    + 分区数据显示大多数分区无积压，单个或个别分区积压严重
    + 跟踪所有消费端实例，发现积压分区没有实例订阅
    + 线上只和jimdb交互的服务**从未出现**过此种情况  

- 发生原因  
_注：以下结论主要来自总结和分析，没有经过严密的验证，单分区积压太难复现_  
    - 实例网络抖动、重启、计算服务抛出异常、长时间无ack等原因可能导致分区消费rebalance，个别情况下新分配的结果会导致有分区没有分配到消费者。  
    - 位点批量提交周期越长、计算服务逻辑越复杂、计算周期越长、计算服务越不稳定越容易出现这种情况。  
    - 推测只写jimdb的交互服务从未出现这种场景的原因就是服务稳定、计算效率极高、几乎没有异常、位点提交频率也非常频繁和稳定。

- 解决方案
    + 消费实例数据和分区数量一致的服务极少碰上此种情况，消费实例少于分区数的情况最多见
    + 提高计算服务稳定性，越少rebalance越不容易发生
    + 控制位点批量提交的大小和周期，设定一个合理的值

#### 分区消费严重不均匀
- 线上现象
    + 分区数据积压呈现较为明显的梯度
    + 分区数据积压和分区编号大小成正比或者反比
    + 单个消费实例接入了多个数据源
    + 计算服务通常有较为复杂的计算逻辑
    + 某个实例**一定**消费了每个数据源的最小分区(或者最大分区)
- 产生原因
    + 实例根据分区编号顺序获取未消费分区
    + 多个数据源的分区数量不同
    + 所有数据源的分区根据消费端实例启动顺序逐一分配
    + 最终，分区的分配不是均匀的，而是和实例启动时间相关的
- 解决方案
    + 最好方案：一个实例消费一个数据源，一劳永逸
    + 单个实例消费多个数据源，通过一定随机算法获取分区，和分区编号错开

#### 爆发性虚假积压
- 线上现象
    + 正常消费的topic在某个时间段呈现爆发性的增长
    + 积压数据经过一段时间断崖式回落
    + 日志显示位点提交频率极低
    + 积压场景出现时间相对有规律
- 发生原因  
    a. 消费端在数据过滤中直接抛弃对计算逻辑无用的数据  
    b. 上游大批量生产对计算逻辑无用的数据，最常见的情况是生产数据库做数据结转  
    c. 位点批量提交的时间点和数量两个条件都依靠有效数据触发判断  
    d. 实际上数据没有积压，只是由于监控通过位点提交判断积压情况，长时间没有位点提交误导了监控，发出积压告警
- 解决方案
    + 无论数据是否有用，都不能跳过位点批量提交的ack计数和计时逻辑

#### 分区漂移
- 线上现象
    + 某个正常消费的分区突然积压
    + 跟踪日志或者消费标记，会发现积压的数据都是消费过的
    + 积压情况和特定分区及消费实例无关，但是有一定周期性
    + 计算服务通常有比较耗时的操作
    + 计算服务中通常存在多线程操作
- 发生原因
    + 计算服务的某个分支中，计算逻辑比较耗时或可能阻塞，同时位点提交操作在该分支存在调用两次的可能
    + 或者计算服务中存在多线程，并且线程中调用了位点提交操作
    + 一个已经被提交过的旧位点，在延迟很久之后被提交成功
    + 最终，kafka服务端将分区的位点回滚到了一个旧的位置
- 解决方案
    + 检查分支条件，ack请求在每个分支里只能有一次
    + 多线程情况下做好线程安全的检查
    + ack操作的时候最好加入最新位点的比对检查
    + 有些操作看似效率较高，在特殊场景下是可能消耗较长时间的  
    比如es提交请求：大促期间由于服务器压力较大且写入量增大，个别写操作会出现版本冲突导致重试，由于频率不高并不直接影响积压，但是如果在写入操作后做ack，又没有做好最新位点的管理，就会出现分区漂移，位点被回滚。
