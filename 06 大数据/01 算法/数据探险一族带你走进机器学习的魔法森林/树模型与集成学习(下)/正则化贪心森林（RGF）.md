# **正则化贪心森林（RGF，Regularized Greedy Forests）** #

RGF是在原理框架上对GBDT进行了改进，在部分数据集上也取得了优于XGBoost的表现，在不考虑计算代价的时候，可以与XGBoost做集成模型。相比于XGBoost，RGF的缺点是计算时间更长。

## **算法原理：** ##

RGF涉及的背景知识有点多，这里只给大家讲一下算法大体框架。
RGF 的核心框架只有两步，思想类似EM算法：

1.  优化当前森林的结构，以获得最小化损失函数的新森林；
2.  优化整个森林的叶子权重，以最小化损失函数；	
3.  不断重复上面两步优化，直到收敛。

RGF对GBDT的改进点：

1. 	GBDT缺少对模型复杂度的控制，RGF与XGBoost一样，通过对损失函数添加正则化项来改进；
2.	GBDT的学习率对模型表现很重要，通常我们需要个较小的学习率，这就会导致需要大量的树，模型复杂度较高。GBDT每步只调整当前树的权重，RGF通过第二步优化中改进所有叶子节点的权重来改进这个问题；
3.	GBDT每步只把前面的模型当做一个黑箱，只取结果，再建立一颗新树去增大错误样本的权重来改进结果。RGF在第一步优化过程中是对整个森林所有树的结构进行优化，寻求全局最优结构。

## **优化算法：** ##

从上面的介绍，我们可以看到，RGF是对GBDT算法思想上的一种优化，并没有限制具体优化算法。当然，文章也给出了一些比较高效的方法。

1.寻找最优结构（第一步优化）：

	可以想象，搜索整个森林代价相当昂贵。为了计算效率，在搜索森林时只考虑两种操作（见下图）：

    （1）分割最新增长的一个节点

    （2）生长一颗新树

![](http://i1.go2yd.com/image.php?url=0IM3DWzabd)


2.权重优化（第二步优化）：

    通过牛顿法优化节点权重。
    
实际训练中，通常新增了多个节点（eg：100）后，才进行一次权重的优化（参考GAN）。


