## 机器学习算法周报----决策树（Decision Tree） ##


决策树(Decision Tree)是一种有监督的预测模型，它是一种树形结构的判别模型。决策树的首节点为根节点，树内节点记录了使用哪个特征来进行类别的判断，每个叶子节点则代表了最后判断的类别。根节点到每个叶节点均形成一条路径规则。而对新的样本进行测试时，只需要从根节点开始，在每个分支节点进行测试，沿着相应的分支递归地进入子树再测试，一直到达叶节点，该叶子节点所代表的类别即是当前测试样本的预测类别。
图1显示为女网友是否会见男网友的决策树预测模型，其中年龄、长相、收入、是否公务员为男网友的属性，绿色节点为树内节点，代表在以上属性上的判别条件；橙色节点为叶节点，代表其预测的类别，即见或不见。决策树模型根据男网友的属性确定其从根节点到叶节点的路径，路径的终点代表最终的预测结果。
 
![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/1.png)

图1女网友见男网友的决策树模型

##1、决策树训练算法的核心问题：##

从样本数据构建决策树模型为决策树的训练过程，决策树的训练算法是一种基于实例的归纳学习方法，从有类别标签的训练数据集中提炼树模型。决策树的训练算法采用一定规则或标准递归分割样本集，采用自顶向下的贪婪搜索遍历可能的决策空间，直到每个子集只包含属于同一类的样本或达到终止条件时停止，在训练过程中，每个分割节点需要保存好分裂变量、分裂点和分类的属性号；在测试阶段，将测试样本从根节点开始进行判别，根据训练时各节点保留的信息，依次路由该样本属于哪个子节点，同样递归地执行下去，直到该样本被分到叶节点中为止，而此时该样本就属于当前叶节点的类别。


决策树的训练算法涉及两个核心问题：①按什么样的次序来选择特征（属性）？②最佳分离点（连续情形）在哪儿？我们以表1数据为例，依次说明决策树的训练算法ID3、C4.5、CART算法的计算流程。

表1 AllElectronics顾客数据库标记类的训练元组

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/2.png)

##2、ID3算法：##
ID3由Ross Quinlan在1986年提出。ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。在ID3中，每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分，也就是说如果一个特征有4种取值，数据将被切分4份，一旦按某特征切分后，该特征在之后的算法执行中，将不再起作用，所以有观点认为这种切分方式过于迅速。信息熵是信息论里面的概念，是信息的度量方式，不确定度越大或者说越混乱，熵就越大。在建立决策树的过程中，根据特征属性划分数据，使得原本“混乱”的数据的熵(混乱度)减少，按照不同特征划分数据熵减少的程度会不一样。在ID3中选择熵减少程度最大的特征来划分数据（贪心），也就是“最大信息熵增益”原则。

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/3.png)

针对表1中的数据，以age作为分裂变量，计算信息增益，计算过程如下：

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/4.png)

当选择age特征进行分裂后的结果如图2所示

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/5.png)

图2选择age特征对根节点进行分裂
##3、C4.5算法：##

C4.5算法是对ID3算法的改进。 ID3算法采用信息增益作为特征重要性的度量存在明显的缺点，它倾向于选择具有较多属性值的特征。信息增益反映系统不确定性的减少程度，对原始样本集划分的越细，信息增益倾向于越大。为了弥补上述不足，C4.5算法中采用信息增益率作为选择分支的准则。信息增益率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的特征。除此之外，C4.5还弥补了ID3中不能处理特征属性值连续的问题。但是，对连续属性值需要进行扫描排序，会使C4.5性能下降。

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/6.png)

针对表1中的数据，属性income将数据划分为3个分区，即low、medium和high，分别包含4、6和4个元组。为了计算income的增益率，首先计算分裂信息，得到

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/7.png)

由于，

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/8.png)

因此，

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/9.png)

##4、CART算法：##

ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且内节点都有两个孩子，所以CART的叶节点比内节点多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低基尼指数。

![](https://raw.githubusercontent.com/xuwenfeng0459/image/master/10.png)

##5、结论：##

在数据分析/数据挖掘领域，决策树是一种常见模型，与其它机器学习模型比较，决策树具有算法简单、运行高效、可解释性强等优点，适用于实时预测类的场景。


