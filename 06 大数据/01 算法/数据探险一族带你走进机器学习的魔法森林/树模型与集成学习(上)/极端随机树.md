# 极端随机树(extraTree) #
	极端随机树(extraTree)算法作为随机森林(randomForest)延伸与扩展，与随机森林的算法十分相似，都是由许多决策树构成。
## 1、随机森林(randomForest)定义： ##
	是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。 
## 2、随机森林(randomForest)与极端随机树(extraTree)的主要区别：  ##
	1）randomForest：样本随机、特征随机、参数随机、 模型随机（ID3 ,C4.5）
	2）extraTree： 特征随机、参数随机、模型随机（ID3 ,C4.5）、分裂随机 
	综上，极端随机树相比于随机森林没有了样本随机，增加了分裂随机，其余地方与随机森林一样。

## 3、极端随机树(extraTree)定义： ##
	extraTree与randomForest的定义类似，都是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。不同的是极端随机树相比于随机森林没有了样本随机，即使用所有的样本构造决策树，并增加了分裂随机，从而构造极端随机树分类器。

## 4、随机森林与极端随机树的详细描述： ##

	1）randomForest应用的是Bagging模型,extraTree使用的所有的样本，只是特征是随机选取的，并且分裂是随机的，所以在某种程度上比随机森林得到的结果更加好；
	
	2）randomForest是在一个随机子集内得到最佳分叉属性，而extraTree是完全随机的得到分叉值，从而实现对决策树进行分叉的；
	
	3）extraTree方法比随机森林的随机性更强。以二叉树为例，当特征属性是类别的形式时，随机选择具有某些类别的样本为左分支，而把具有其他类别的样本作为右分支；当特征属性是数值的形式时，随机选择一个处于该特征属性的最大值和最小值之间的任意数，当样本的该特征属性值大于该值时，作为左分支，当小于该值时，作为右分支。这样就实现了在该特征属性下把样本随机分配到两个分支上的目的。然后计算此时的分叉值（如果特征属性是类别的形式，可以应用基尼指数；如果特征属性是数值的形式，可以应用均方误差）。遍历节点内的所有特征属性，按上述方法得到所有特征属性的分叉值，我们选择分叉值最大的那种形式实现对该节点的分叉。由上可知，该方法比随机森林的随机性更强；
	
	4）extraTree对于某棵决策树，由于它的最佳分叉属性是随机选择的，因此用它的预测结果往往是不准确的，但多棵决策树组合在一起，就可以达到很好的预测效果。
	
	5）当extraTree构建好了以后，可以应用全部的训练样本来得到该算法的预测误差。这是因为尽管构建决策树和预测应用的是同一个训练样本集，但由于最佳分叉属性是随机选择的，所以我们仍然会得到完全不同的预测结果，用该预测结果就可以与样本的真实响应值比较，从而得到预测误差。
